---
title: "Introduction to `vimp`"
author: "Brian D. Williamson"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    keep_md: true
vignette: >
  %\VignetteIndexEntry{Introduction to `vimp`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
references:
- id: williamson2020a
  title: Nonparametric variable importance assessment using machine learning techniques
  author:
  - family: Williamson
    given: Brian D
  - family: Gilbert
    given: Peter B
  - family: Carone
    given: Marco
  - family: Simon
    given: Noah
  publisher: Biometrics
  type: article-journal
  issued:
   year: 2020
- id: williamson2020b
  title: A unified approach for inference on algorithm-agnostic variable importance
  author:
  - family: Williamson
    given: Brian D
  - family: Gilbert
    given: Peter B
  - family: Simon
    given: Noah
  - family: Carone
    given: Marco
  publisher: arXiv
  type: article-journal
  issued:
   year: 2020
  URL: https://arxiv.org/abs/2004.03683
- id: williamson2020c
  title: Efficient nonparametric statistical inference on population feature importance using Shapley values
  author:
  - family: Williamson
    given: Brian D
  - family: Feng
    given: Jean
  publisher: arXiv
  type: article-journal
  issued:
   year: 2020
  URL: https://arxiv.org/abs/2006.09481
- id: hastie1990
  title: Generalized Additive Models
  author:
  - family: Hastie
    given: TJ
  - family: Tibshirani
    given: RJ
  #container-title: CRC Press
  volume: 43
  #URL: 'http://dx.doi.org/10.1038/nmat3283'
  #DOI: 10.1038/nmat3283
  #issue: 4
  publisher: CRC Press
  #page: 261-263
  type: book
  issued:
    year: 1990
    #month: 3
- id: hastie2009
  title: The Elements of Statistical Learning
  author:
  - family: Hastie
    given: T
  - family: Tibshirani
    given: R
  - family: Friedman
    given: J
  type: book
  issued:
    year: 2009
- id: vanderlaan2007
  title: Super learner
  author:
  - family: van der Laan
    given: MJ
  - family: Polley
    given: EC
  - family: Hubbard
    given: AE
  volume: 6
  publisher: Statistical Applications in Genetics and Molecular Biology
  type: article-journal
  issued:
   year: 2007
- id: breiman2001
  title: Random forests
  author:
  - family: Breiman
    given: L
  volume: 45
  publisher: Machine Learning
  type: article-journal
  issued:
   year: 2001
- id: friedman2001
  title: "Greedy function approximation: a gradient boosting machine"
  author:
  - family: Friedman
    given: JH
  publisher: The Annals of Applied Statistics
  type: article-journal
  issued:
   year: 2001
- id: zou2005
  title: Regularization and variable selection via the elastic net
  author:
  - family: Zou
    given: H
  - family: Hastie
    given: TJ
  publisher: "Journal of the Royal Statistical Society: Series B (Statistical Methodology)"
  type: article-journal
  issued:
   year: 2005
---

```{r setup, echo = FALSE, include = FALSE}
library(knitr)
opts_knit$set(cache = FALSE, verbose = TRUE, global.par = TRUE)
```

```{r more-setup, echo = FALSE}
par(mar = c(5, 12, 4, 2) + 0.1)
```

## Introduction

`vimp` is a package that computes nonparametric estimates of variable importance and provides valid inference on the true importance. The package supports flexible estimation of variable importance based on the difference in nonparametric $R^2$, classification accuracy, and area under the receiver operating characteristic curve (AUC). These quantities are all nonparametric generalizations of the usual measures in simple parametric models (e.g., linear models). For more details, see the accompanying manuscripts @williamson2020a, @williamson2020b, and @williamson2020c.

Variable importance estimates may be computed quickly, depending on the techniques used to estimate the underlying conditional means --- if these techniques are slow, then the variable importance procedure will be slow.

The code can handle arbitrary dimensions of features, and may be used to estimate the importance of any single feature or group of features for predicting the outcome. The package also includes functions for cross-validated importance.

The author and maintainer of the `vimp` package is [Brian Williamson](https://bdwilliamson.github.io/). The methods implemented here have also been implemented in Python under the package [`vimpy`](https://github.com/bdwilliamson/vimpy).

## Installation

A stable version of the package may be downloaded and installed from CRAN. Type the following command in your R console to install the stable version of `vimp`:
```{r install-vimp, eval = FALSE}
install.packages("vimp")
```

A development version of the package may be downloaded and installed from GitHub using the `devtools` package. Type the following command in your R console to install the development version of `vimp`:

```{r devtools-install-vimp, eval = FALSE}
# only run if you don't have devtools
# previously installed
# install.packages("devtools")
devtools::install_github("bdwilliamson/vimp")
```

## Quick Start

This section should serve as a quick guide to using the `vimp` package --- we will cover the main functions for estimating $R^2$-based variable importance using a simulated data example. More details are given in the next section.

First, load the `vimp` package:
```{r load-vimp, message = FALSE}
library("vimp")
```

Next, create some data:
```{r gen-data}
# -------------------------------------------------------------
# problem setup
# -------------------------------------------------------------
# set up the data
set.seed(5678910)
n <- 1000
p <- 2
s <- 1 # desire importance for X_1
x <- data.frame(replicate(p, runif(n, -1, 1)))
y <- (x[,1])^2*(x[,1]+7/5) + (25/9)*(x[,2])^2 + rnorm(n, 0, 1)
# set up folds for hypothesis testing
folds <- sample(rep(seq_len(2), length = length(y)))
```

This creates a matrix of covariates `x` with two columns, a vector `y` of normally-distributed outcome values, and a set of folds for a sample of `n = 100` study participants.

The workhorse function of `vimp`, for $R^2$-based variable importance, is `vimp_rsquared`. There are two ways to compute variable importance: in the first method, you allow `vimp` to run regressions for you and return variable importance; in the second method (discussed in ["Using precomputed regression function estimates in `vimp`"](precomputed-regressions.html)), you run the regressions yourself and plug these into `vimp`. I will focus on the first method here. The basic arguments are

* Y: the outcome (in this example, `y`)
* X: the covariates (in this example, `x`)
* indx: the covariate(s) of interest for evaluating importance (here, either 1 or 2)
* run_regression: a logical value telling `vimp_rsquared` whether or not to run a regression of Y on X (`TRUE` in this example)
* SL.library: a "library" of learners to pass to the function `SuperLearner` (since `run_regression = TRUE`)
* V: the number of folds to use for cross-fitted variable importance

This second-to-last argument, `SL.library`, determines the estimators you want to use for the conditional mean of Y given X. Estimates of variable importance rely on good estimators of the conditional mean, so we suggest using flexible estimators and model stacking to do so. One option for this is the `SuperLearner` package; load that package using

```{r learner-lib-small, message = FALSE}
library("SuperLearner")
# load specific algorithms
library("ranger")
```

The code
```{r est-1}
est_1 <- vimp_rsquared(Y = y, X = x, indx = 1, run_regression = TRUE, 
                       SL.library = c("SL.ranger", "SL.mean"), V = 2,
                       env = environment())
```

uses the Super Learner to fit the required regression functions, and computes an estimate of variable importance for the importance of $X_1$. We can visualize the estimate, standard error, and confidence interval by printing or typing the object name:
```{r print-est-1}
est_1
print(est_1)
```

This output shows that we have estimated the importance of $X_1$ to be `r round(est_1$est, 3)`, with a 95% confidence interval of `r paste0("[", round(est_1$ci[,1], 3), ", ", round(est_1$ci[, 2], 3), "]")`.

## Detailed guide

In this section, we provide a fuller example of estimating $R^2$-based variable importance in the context of the South African heart disease study data [@hastie2009].

Often when working with data we attempt to estimate the conditional mean of the outcome $Y$ given features $X$, defined as $\mu_P(x) = E_P(Y \mid X = x)$.

There are many tools for estimating this conditional mean. We might choose a classical parametric tool such as linear regression. We might also want to be model-agnostic and use a more nonparametric approach to estimate the conditional mean. However,

- This involves using some nonparametric smoothing technique, which requires: (1) choosing a technique, and (2) selecting tuning parameters
- Naive optimal tuning balances out the bias and variance of the smoothing estimator. Is this the correct trade-off for estimating the conditional mean?

Once we have a good estimate of the conditional mean, it is often of scientific interest to understand which features contribute the most to the variation in $\mu_P$. Specifically, we might consider \[\mu_{P, s}(x) = E_P(Y \mid X_{(-s)} = x_{(-s)}),\] where for a vector $v$ and a set of indices $s$, $v_{-(s)}$ denotes the elements of $v$ with index not in $s$. By comparing $\mu_{P, s}$ to $\mu_P$ we can evaluate the importance of the $s$th element (or group of elements).

Assume that our data are generated according to the mechanism $P_0$. We define the population $R^2$ value of a given regression function $\mu$ as $R^2(\mu, P_0) = 1 - \frac{E_{P_0}\{Y - \mu(X)\}^2}{var_{P_0}(Y)}$, where the numerator of this expression is the population mean squared error and the denominator is the population variance. We can then define a nonparametric measure of variable importance, \[\psi_{0, s} = R^2(\mu_{P_0}, P_0) - R^2(\mu_{P_0,s}, P_0),\] which is the proportion of the variability in the outcome explained by including $X_j$ in our chosen estimation technique.

This document introduces you to the basic tools in `vimp` and how to apply them to a dataset. I will explore one method for obtaining variable estimates using `vimp`: you only specify a *library* of candidate estimators for the conditional means $\mu_{P_0}$ and $\mu_{P_0, s}$; you allow `vimp` to obtain the optimal estimates of these quantities using the `SuperLearner` [@vanderlaan2007], and use these estimates to obtain variable importance estimates. A second method (using precomputed estimates of the regression functions) exists and is described in ["Using precomputed regression function estimates in `vimp`"](precomputed-regressions.html).

### A look at the South African heart disease study data

Throughout this document I will use the South African heart disease study data [@hastie2009], freely available from the [Elements of Statistical Learning website](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data). Information about these data is available [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt).

```{r load-heart-data}
# read in the data from the Elements website
library("RCurl")
heart_data <- read.csv(text = getURL("http://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data"), header = TRUE, stringsAsFactors = FALSE)
# minor data cleaning
heart <- heart_data[, 2:dim(heart_data)[2]]
heart$famhist <- ifelse(heart$famhist == "Present", 1, 0)
# sample-splitting folds for hypothesis testing
heart_folds <- sample(rep(seq_len(2), length = dim(heart)[1]))
```

In addition to the indicator of myocardial infarction `chd`, the outcome of interest, there are measurements on two groups of variables. First are behavioral features: cumulative tobacco consumption, current alcohol consumption, and type A behavior (a behavioral pattern linked to stress). Second are biological features: systolic blood pressure, low-density lipoprotein (LDL) cholesterol, adiposity (similar to body mass index), family history of heart disease, obesity, and age.

Since there are nine features and two groups, it is of interest to determine variable importance both for the nine individual features separately and for the two groups of features.

### A first approach: linear regression
Suppose that I believe that a linear model truly describes the relationship between the outcome and the covariates in these data. In that case, I would be justified in only fitting a linear regression to estimate the conditional means; this means that in my importance analysis, I should also use only linear regression. Note that because I chose to fit the regression functions first and then plug them into `vimp`, I need to explicitly specify the folds used for hypothesis testing. The analysis is achieved by the following:

```{r est-heart-regressions-lm}
X <- heart[, -ncol(heart)]
lm_vim_sbp <- vim(Y = heart$chd, X = X, indx = 1, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_tob <- vim(Y = heart$chd, X = X, indx = 2, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_ldl <- vim(Y = heart$chd, X = X, indx = 3, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_adi <- vim(Y = heart$chd, X = X, indx = 4, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_fam <- vim(Y = heart$chd, X = X, indx = 5, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_tpa <- vim(Y = heart$chd, X = X, indx = 6, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_obe <- vim(Y = heart$chd, X = X, indx = 7, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_alc <- vim(Y = heart$chd, X = X, indx = 8, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")
lm_vim_age <- vim(Y = heart$chd, X = X, indx = 9, run_regression = TRUE, SL.library = "SL.lm", type = "r_squared")

# make a table with the estimates using the merge_vim() function
library("dplyr")
lm_mat <- merge_vim(lm_vim_sbp, lm_vim_tob, lm_vim_ldl, lm_vim_adi,
                lm_vim_fam, lm_vim_tpa, lm_vim_obe, lm_vim_alc, lm_vim_age)
# print out the matrix
lm_mat
```

### Building a library of learners

In general, we don't believe that a linear model truly holds. Thinking about potential model misspecification leads us to consider other algorithms. Suppose that I prefer to use generalized additive models [@hastie1990] to estimate $\mu_{P_0}$ and $\mu_{P_0, s}$, so I am planning on using the `gam` package. Suppose that you prefer to use the elastic net [@zou2005], and are planning to use the `glmnet` package.

The choice of either method is somewhat subjective, and I also will have to use a technique like cross-validation to determine an optimal tuning parameter in each case. It is also possible that neither additive models nor the elastic net will do a good job estimating the true conditional means!

This motivates using `SuperLearner` to allow the data to determine the optimal combination of *base learners* from a *library* that I define. These base learners are a combination of different methods (e.g., generalized additive models and elastic net) and instances of the same method with different tuning parameter values (e.g., additive models with 3 and 4 degrees of freedom). The Super Learner is an example of model stacking, or model aggregation --- these approaches use a data-adaptive combination of base learners to make predictions.

For instance, my library could include the elastic net, random forests [@breiman2001], and gradient boosted trees [@friedman2001] as follows:

```{r full-learner-lib}
# create a function for boosted stumps
SL.gbm.1 <- function(..., interaction.depth = 1) SL.gbm(..., interaction.depth = interaction.depth)

# create GAMs with different degrees of freedom
SL.gam.3 <- function(..., deg.gam = 3) SL.gam(..., deg.gam = deg.gam)
SL.gam.4 <- function(..., deg.gam = 4) SL.gam(..., deg.gam = deg.gam)
SL.gam.5 <- function(..., deg.gam = 5) SL.gam(..., deg.gam = deg.gam)

# add more levels of alpha for glmnet
create.SL.glmnet <- function(alpha = c(0.25, 0.5, 0.75)) {
  for (mm in seq(length(alpha))) {
    eval(parse(file = "", text = paste('SL.glmnet.', alpha[mm], '<- function(..., alpha = ', alpha[mm], ') SL.glmnet(..., alpha = alpha)', sep = '')), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create.SL.glmnet()

# add tuning parameters for randomForest
create.SL.randomForest <- function(tune = list(mtry = c(1, 5, 7), nodesize = c(1, 5, 10))) {
  tuneGrid <- expand.grid(tune, stringsAsFactors = FALSE)
  for (mm in seq(nrow(tuneGrid))) {
    eval(parse(file = "", text = paste("SL.randomForest.", mm, "<- function(..., mtry = ", tuneGrid[mm, 1], ", nodesize = ", tuneGrid[mm, 2], ") SL.randomForest(..., mtry = mtry, nodesize = nodesize)", sep = "")), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create.SL.randomForest()

# create the library
learners <- c("SL.glmnet", "SL.glmnet.0.25", "SL.glmnet.0.5", "SL.glmnet.0.75",
              "SL.randomForest", "SL.randomForest.1", "SL.randomForest.2", "SL.randomForest.3",
              "SL.randomForest.4", "SL.randomForest.5", "SL.randomForest.6", "SL.randomForest.7",
              "SL.randomForest.8", "SL.randomForest.9",
              "SL.gbm.1")
```

Now that I have created the library of learners, I can move on to estimating variable importance.

### Estimating variable importance for a single variable

The main function for R-squared-based variable importance in the `vimp` package is the `vimp_rsquared()` function. There are five main arguments to `vimp_rsquared()`:

- `Y`, the outcome
- `X`, the covariates
- `indx`, which determines the feature I want to estimate variable importance for
- `SL.library`, the library of candidate learners
- `V`, the number of cross-validation folds to use for computing variable importance

The main arguments differ if precomputed regression function estimates are used; please see ["Using precomputed regression function estimates in `vimp`"](precomputed-regressions.html) for further discussion of this case.

Suppose that the first feature that I want to estimate variable importance for is family history of heart disease, `fam`. Then supplying `vimp_rsquared()` with

- `Y = heart$chd`
- `X = X`
- `indx = 5`
- `SL.library = learners`
- `V = 5`

means that:

- I want to use `SuperLearner()` to estimate the conditional means $\mu_{P_0}$ and $\mu_{P_0,s}$, and my candidate library is `learners`
- I want to estimate variable importance for the fifth column of the South African heart disease study covariates, which is `fam`
- I want to use five-fold cross-fitting to estimate importance

The call to `vimp_rsquared()` looks like this:
```{r vimp-with-sl-1, eval = FALSE}
vimp_rsquared(Y = heart$chd, X = X,
    indx = 5, run_regression = TRUE, SL.library = learners, V = 5)
```

While this is the preferred method for estimating variable importance, using a large library of learners may cause the function to take time to run. Usually this is okay --- in general, you took a long time to collect the data, so letting an algorithm run for a few hours should not be an issue.

However, for the sake of illustration, I can estimate varibable importance for family history only using only using a small library, a small number of cross-validation folds in the Super Learner, and a small number of cross-fitting folds as follows (again, I suggest using a larger number of folds and a larger library in practice):
```{r vimp-with-sl-fam, message = FALSE}
# small learners library
learners.2 <- c("SL.ranger")
# small number of cross-fitting folds
V <- 2
# small number of CV folds for Super Learner
sl_cvcontrol <- list(V = 2)

# now estimate variable importance
start_time <- Sys.time()
fam_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 5, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
end_time <- Sys.time()
```

This code takes approximately `r round(as.numeric(end_time - start_time), 3)` seconds to run on a (not very fast) PC. I can display these estimates:

```{r print-fam-vim}
fam_vim
```

The object returned by `vimp_rsquared()` also contains fitted values from using `SuperLearner()`; I access these using `$full_fit` and `$red_fit`. For example,

```{r look-at-fam-ests}
head(fam_vim$full_fit)
head(fam_vim$red_fit)
```

I can obtain estimates for the remaining individual features in the same way (again using only using a small library for illustration):
```{r heart-sl}
# estimate variable importance
tpa_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 6, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
alc_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 8, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
sbp_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 1, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
tob_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 2, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
ldl_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 3, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
adi_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 4, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
obe_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 7, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
age_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = 9, SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
```

Now that I have estimates of each of individual feature's variable importance, I can view them all simultaneously by plotting:
```{r heart-vim, fig.width = 8.5, fig.height = 8, message = FALSE}
library("dplyr")
library("tibble")
library("ggplot2")
library("cowplot")
theme_set(theme_cowplot())
# combine the objects together
ests <- merge_vim(sbp_vim, tob_vim, ldl_vim, adi_vim,
                fam_vim, tpa_vim, obe_vim, alc_vim, age_vim)
all_vars <- c("Sys. blood press.", "Tobacco consump.", "LDL cholesterol",
              "Adiposity", "Family history", "Type A behavior", "Obesity",
              "Alcohol consump.", "Age")

est_plot_tib <- ests$mat %>%
  mutate(
    var_fct = rev(factor(s, levels = ests$mat$s,
                     labels = all_vars[as.numeric(ests$mat$s)],
                     ordered = TRUE))
  )

# plot
est_plot_tib %>%
  ggplot(aes(x = est, y = var_fct)) +
  geom_point() +
  geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
  xlab(expression(paste("Variable importance estimates: ", R^2, sep = ""))) +
  ylab("") +
  ggtitle("Estimated individual feature importance") +
  labs(subtitle = "in the South African heart disease study data")
```

## Estimating variable importance for a group of variables

Now that I have estimated variable importance for each of the individual features, I can estimate variable importance for each of the groups that I mentioned above: biological and behavioral features.

The only difference between estimating variable importance for a group of features rather than an individual feature is that now I specify a vector for `s`; I can use any of the options listed in the previous section to compute these estimates.

```{r heart-group-vim, fig.width = 8.5, fig.height = 8}
# get the estimates
behav_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = c(2, 6, 8), SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)
bios_vim <- vimp_rsquared(Y = heart$chd, X = X, indx = c(1, 3, 4, 5, 7, 9), SL.library = learners.2, na.rm = TRUE, env = environment(), V = V, cvControl = sl_cvcontrol)

# combine and plot
groups <- merge_vim(behav_vim, bios_vim)
all_grp_nms <- c("Behavioral features", "Biological features")

grp_plot_tib <- groups$mat %>%
  mutate(
    grp_fct = factor(case_when(
      s == "2,6,8" ~ "1",
      s == "1,3,4,5,7,9" ~ "2"
    ), levels = c("1", "2"),  labels = all_grp_nms, ordered = TRUE)
  )
grp_plot_tib %>%
  ggplot(aes(x = est, y = grp_fct)) +
  geom_point() +
  geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
  xlab(expression(paste("Variable importance estimates: ", R^2, sep = ""))) +
  ylab("") +
  ggtitle("Estimated feature group importance") +
  labs(subtitle = "in the South African heart disease study data")
```

## Types of population variable importance

In this document, I have focused on one particular definition of population variable importance that I call *conditional* variable importance. For a further discussion of what I call *marginal* variable importance and *Shapley population* variable importance, please see ["Types of VIMs"](types-of-vims.html).

## References
